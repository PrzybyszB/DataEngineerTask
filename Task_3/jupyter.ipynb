{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d6d4e5",
   "metadata": {},
   "source": [
    "### Download data and upload to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from os.path import basename\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "urls = {\n",
    "    \"yellow\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\",\n",
    "    \"green\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\",\n",
    "    \"zones\": \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "}\n",
    "\n",
    "bucket_name = os.getenv(\"BUCKET_NAME\")\n",
    "s3_prefix = os.getenv(\"S3_RAW_PREFIX\")\n",
    "\n",
    "if not bucket_name:\n",
    "    raise ValueError(\"BUCKET_NAME not set in enviroment\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "for name, url in urls.items():\n",
    "    response= requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    response.raw.decode_content = True\n",
    "\n",
    "    filename = basename(url)\n",
    "    s3_key = os.path.join(s3_prefix, filename)\n",
    "\n",
    "    s3.upload_fileobj(response.raw, bucket_name, s3_key)\n",
    "    print(f\"Uploaded {filename} to s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "print(f\"All url was successfully uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e8ba7",
   "metadata": {},
   "source": [
    "### Create Spark connection with S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c347b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.hadoop:hadoop-aws:3.3.1,\"\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.11.375,\"\n",
    "    \"org.postgresql:postgresql:42.2.27 pyspark-shell\"\n",
    ")\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "def create_spark_session(app_name=\"S3 CSV Reader\"):\n",
    "    spark_conf = {\n",
    "        \"spark.jars.packages\": \"org.apache.hadoop:hadoop-aws:3.3.1,\"\n",
    "                              \"com.amazonaws:aws-java-sdk-bundle:1.11.375\",\n",
    "        \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\",\n",
    "        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "        \"spark.sparkContext.setLogLevel\" :\"INFO\"\n",
    "        \n",
    "    }\n",
    "\n",
    "    builder = SparkSession.builder.appName(app_name)\n",
    "    for k, v in spark_conf.items():\n",
    "        builder = builder.config(k, v)\n",
    "    \n",
    "    spark = builder.getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1603f00",
   "metadata": {},
   "source": [
    "### Check that we can read all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f30be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df = spark.read.parquet(\"s3a://mypracawsbucketsc2/raw/yellow_tripdata_2023-01.parquet\")\n",
    "yellow_df.show()\n",
    "yellow_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79955db",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df = spark.read.parquet(\"s3a://mypracawsbucketsc2/raw/green_tripdata_2023-01.parquet\")\n",
    "green_df.show()\n",
    "green_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = spark.read.csv(\"s3a://mypracawsbucketsc2/raw/taxi+_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "taxi_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c669919",
   "metadata": {},
   "source": [
    "### Join PULocation, DOLocation with taxi zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a in memory temp-view for df\n",
    "taxi_df.createOrReplaceGlobalTempView('taxi_zone')\n",
    "green_df.createOrReplaceGlobalTempView('green_taxi')\n",
    "yellow_df.createOrReplaceGlobalTempView('yellow_taxi')\n",
    "\n",
    "SQL_QUERY = \"\"\"\n",
    "    \n",
    "WITH g_cte AS (\n",
    "        SELECT \n",
    "        gt.PULocationID AS pick_up_location, \n",
    "        gt.DOLocationID AS drop_off_location,\n",
    "        g_tz1.zone AS pick_up_zone,\n",
    "        g_tz2.zone AS drop_off_zone,\n",
    "        CONCAT(gt.PULocationID, '_', gt.DOLocationID) AS route_id,\n",
    "        (UNIX_TIMESTAMP(lpep_dropoff_datetime) - UNIX_TIMESTAMP(lpep_pickup_datetime)) / 60 AS trip_duration,\n",
    "        trip_distance,\n",
    "        'green' AS taxi_type\n",
    "        FROM global_temp.green_taxi gt\n",
    "        LEFT JOIN global_temp.taxi_zone g_tz1 ON gt.PULocationID = g_tz1.LocationID\n",
    "        LEFT JOIN global_temp.taxi_zone g_tz2 ON gt.DOLocationID= g_tz2.LocationID\n",
    "    ), \n",
    "    y_cte AS (\n",
    "        SELECT \n",
    "        yt.PULocationID AS pick_up_location, \n",
    "        yt.DOLocationID AS drop_off_location,\n",
    "        y_tz1.zone AS pick_up_zone,\n",
    "        y_tz2.zone AS drop_off_zone,\n",
    "        CONCAT(yt.PULocationID, '_', yt.DOLocationID) AS route_id,\n",
    "        (UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 60 AS trip_duration,\n",
    "        trip_distance,\n",
    "        'yellow' AS taxi_type\n",
    "        FROM global_temp.yellow_taxi yt\n",
    "        LEFT JOIN global_temp.taxi_zone y_tz1 ON yt.PULocationID = y_tz1.LocationID\n",
    "        LEFT JOIN global_temp.taxi_zone y_tz2 ON yt.DOLocationID= y_tz2.LocationID\n",
    "    )\n",
    "    SELECT \n",
    "        pick_up_zone,\n",
    "        drop_off_zone, \n",
    "        route_id, \n",
    "        COUNT(route_id) as total_trips, \n",
    "        ROUND(AVG(trip_duration),2) AS avg_trip_duration,\n",
    "        ROUND(AVG(trip_distance),2) AS avg_trip_distance,\n",
    "        taxi_type\n",
    "    FROM g_cte\n",
    "    GROUP BY route_id, pick_up_zone, drop_off_zone, taxi_type\n",
    "    \n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        pick_up_zone,\n",
    "        drop_off_zone, \n",
    "        route_id, \n",
    "        COUNT(route_id) as total_trips, \n",
    "        ROUND(AVG(trip_duration),2) AS avg_trip_duration,\n",
    "        ROUND(AVG(trip_distance),2) AS avg_trip_distance,\n",
    "        taxi_type\n",
    "    FROM y_cte\n",
    "    GROUP BY route_id, pick_up_zone, drop_off_zone, taxi_type\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "transformed_df = spark.sql(SQL_QUERY)\n",
    "transformed_df.show()\n",
    "transformed_df.orderBy(F.rand()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02c221",
   "metadata": {},
   "source": [
    "### Saving to as parquet to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "s3_prefix = os.getenv(\"S3_PROCESSED_PREFIX\")\n",
    "bucket_name = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "transformed_df.write\\\n",
    "    .mode(\"append\") \\\n",
    "    .parquet(f\"s3a://{bucket_name}/{s3_prefix}/task_3_data_parquet/\")\n",
    "\n",
    "print(f\"Data successfully written to s3://{bucket_name}/{s3_prefix}task_3_data_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e467d7",
   "metadata": {},
   "source": [
    "### Data from ATHENE\n",
    "1. Confirm that the number of trips is greater than zero.  \n",
    "\n",
    "SQ_QUERY =  SELECT pick_up_zone, drop_off_zone, total_trips\n",
    "            FROM \"task_3_glue_db\".\"task_3_data_parquet\"\n",
    "            WHERE total_trips > 0\n",
    "            ORDER BY total_trips ASC\n",
    "            LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee642f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"pick_up_zone\": [\n",
    "        \"Jackson Heights\", \"Sunset Park East\", \"Rosedale\", \"Bay Ridge\",\n",
    "        \"Sheepshead Bay\", \"Old Astoria\", \"Brooklyn Heights\",\n",
    "        \"Briarwood/Jamaica Hills\", \"North Corona\", \"Steinway\"\n",
    "    ],\n",
    "    \"drop_off_zone\": [\n",
    "        \"East Chelsea\", \"Central Harlem North\", \"Hillcrest/Pomonok\", \"Dyker Heights\",\n",
    "        \"JFK Airport\", \"Yorkville West\", \"Lincoln Square West\",\n",
    "        \"JFK Airport\", \"Briarwood/Jamaica Hills\", \"East Harlem South\"\n",
    "    ],\n",
    "    \"total_trips\": [1,1,1,1,1,1,1,1,1,1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5b6fd",
   "metadata": {},
   "source": [
    "### Data from ATHENE\n",
    "2. Get statistics like the top 10 zone pairs with the highest trip counts.\n",
    "\n",
    "SQL_QUERY=  SELECT pick_up_zone, drop_off_zone, total_trips\n",
    "            FROM \"task_3_glue_db\".\"task_3_data_parquet\"\n",
    "            GROUP BY pick_up_zone, drop_off_zone, total_trips\n",
    "            ORDER BY total_trips desc\n",
    "            LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"pick_up_zone\": [\n",
    "        \"Upper East Side South\", \"Upper East Side North\", \"N/A\", \"Upper East Side North\",\n",
    "        \"Upper East Side South\", \"Upper East Side South\", \"Midtown Center\",\n",
    "        \"Midtown Center\", \"Lenox Hill West\", \"Lincoln Square East\"\n",
    "    ],\n",
    "    \"drop_off_zone\": [\n",
    "        \"Upper East Side North\", \"Upper East Side South\", \"N/A\", \"Upper East Side North\",\n",
    "        \"Upper East Side South\", \"Midtown Center\", \"Upper East Side South\",\n",
    "        \"Upper East Side North\", \"Upper East Side North\", \"Upper West Side South\"\n",
    "    ],\n",
    "    \"total_trips\": [22303, 18981, 15354, 14926, 14546, 9408, 9320, 8599, 8299, 8198]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
