# Data Engineering Tasks Collection

This repository contains a collection of tasks and exercises I completed as part of my learning journey to become a Data Engineer.

## Purpose

I created this repository to document and organize the tasks I solve while practicing data engineering skills. I use ChatGPT to generate task descriptions based on real-world data engineering scenarios. Each task is stored in a dedicated folder and includes:
- A task description (written in English),
- My implementation or solution (in Python, SQL, or other relevant technologies),
- Any additional notes or explanations if necessary.

## Tech Stack

The tasks in this repository involve the following tools and technologies:

- **Python** – for scripting and data processing
- **Pandas** – for in-memory data manipulation and analysis
- **PySpark / Apache Spark** – for distributed data processing
- **Apache Airflow** – for workflow orchestration and pipeline scheduling
- **Docker** – for containerization and isolated development environments
- **AWS** (S3, Athena, Glue, Redshift) – for cloud-based storage, querying, and data integration
- **PostgreSQL** – for relational database operations and SQL practice

## Topics Covered

The tasks span across various areas of data engineering, including but not limited to:
- Data ingestion and transformation (ETL/ELT)
- Data modeling and warehousing
- Working with tools such as Pandas, PySpark, SQL, and Airflow
- Cloud services (e.g. AWS S3, Athena, Redshift)
- Workflow orchestration
- Big Data processing

## How to Navigate

Each folder in the repository contains a single task:
- `README.md` with the task description
- Source code with the solution (e.g. `.py`, `.sql`)
- Supporting files (if any)

This structure allows for easy review, feedback, and iteration over time.

## Notes

This is a personal learning project. I’m continuously updating this repository as I explore new areas and technologies in data engineering. Feedback and suggestions are welcome!
